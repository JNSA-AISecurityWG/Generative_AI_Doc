# L-01:訓練データの汚染
## 概要
生成AIは訓練データから学習したパターンに基づいて出力を生成するため、訓練データの質はモデルの性能に直接影響を及ぼします。訓練データの汚染は、攻撃者が生成AIの訓練データに誤情報や偏向情報などの汚染データを注入し、モデルの性能や信頼性を低下させる攻撃手法です。

## 攻撃例
ここでは、いくつかの訓練データの汚染による攻撃例を挙げます。

### スプリットビュー・データポイズニング（Split-View Data Poisoning）
悪意のある攻撃者が、ドメインが期限切れになったサイトのドメインを購入し、汚染したデータを発信することで、生成AIの訓練データに汚染したデータを注入する攻撃です。
### フロントランニング・データポイズニング（Frontrunning Data Poisoning）
生成AIが訓練データを集めるデータソースを事前に把握し、一時的に汚染したコンテンツをデータソースへ追加することで、生成AIの訓練データに注入する攻撃手法です。

## 対策
単一の対策で100％リスクをなくすことは現実的に困難なため、生成AIを訓練する条件等によって、有効な対策を適宜選択、組み合わせることが重要です。

訓練データの汚染による被害を防ぐためには、以下のような対策が有効です。

* モデル情報の公開制限
  * 「訓練データに何を用いているか」などのメタ情報の公開を制限することで、攻撃者に汚染データの注入をされにくくします。
* 訓練データの正当性やサプライチェーンの検証
  * 特に外部から調達したデータに対して、データソースの信頼性やデータの正当性を検証します。
* 取り込まれるデータのサニタイズ
  * 訓練に用いるデータを精査することで、汚染されたデータを取り除きます。
* モデル自身の頑健性強化
  * 分散学習や正則化手法などを取り入れ、生成AIの頑健性を向上させることで、汚染データの影響を最小限にとどめます。
* モデルの分析やセキュリティ検査の実施
  * 特定の入力に対するモデル内部の挙動分析や、攻撃者視点でのセキュリティ検査により、生成AIのリスクを可視化します。
* 生成AIの応答監視やフィードバックサイクルの導入
  * 望ましくない応答を確認し、改善するフィードバックサイクルを導入することで、汚染データによる影響を減少させます。


## 参考資料
1. OWASP Top 10 for LLM Applications v1.1 - OWASP, https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf
1. Poison Training Data - MITRE Corporation., https://atlas.mitre.org/techniques/AML.T0020
  