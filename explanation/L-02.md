# L-02:モデルの盗難
## 概要
生成AIにおいて、モデルとは大量のデータから学習を行い生成したものであり、様々な入力を処理し、結果を出力するもので非常に重要なものです。
モデルの盗難は、攻撃者が対象である独自の生成AIのモデルに対して不正にアクセスされ、コピーされることやその出力結果から同等のものをつくられてしまうものです。

## 攻撃例
ここでは、いくつかの例を挙げます。
* 生成AIを提供しているシステムの脆弱性を利用し、モデルのリポジトリに不正にアクセスします。
* 攻撃者が細工したリクエストやプロンプトインジェクション等を用いて十分な数の出力を収集し、それをもとにシャドウモデルを作成します。
ただし、モデルの抽出においては、大量な出力が必要となります。また、攻撃者は複製できるものは部分的なものであり、100%複製することは不可能です。


## 対策
モデルの盗難に対する対策は下記の対策があります。
* 生成AIを提供しているシステムにおいて正しくアクセス制御と認証を用いて不正アクセスを防止します。
* 生成AIを提供しているシステム自体に適切な脆弱性管理を行い、システムの脆弱性によるリスクを防止します。
* アクセスログやモデルのリポジトリに対するアクセスをモニタリングし、不正な行動を検知・対応します。
* 生成AIを提供しているシステムのAPIに適切なレートリミットを設定し、データ流出のリスクを低減します。

## 参考資料
* OWASP Top 10 for LLM Applications v1.1 - OWASP
  * https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf
* AI Risk Management Framework - NIST　
  * https://www.nist.gov/itl/ai-risk-management-framework