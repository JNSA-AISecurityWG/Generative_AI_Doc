# L-02:モデルの盗難
## 概要
生成AIにおいてモデルとは、大量のデータを元に膨大なリソースをかけて学習したものであり、非常に重要な資産となり得ます。
モデルの盗難は、攻撃者が対象である生成AIモデルに不正アクセスすることで盗み出す行為、またはモデルの入出力結果をもとに、同等のものを手元に作成してしまう行為を指します。

## 攻撃例
ここでは、いくつかの例を挙げます。
* 生成AIを提供しているシステムの脆弱性を利用して、モデルファイルに不正アクセスし盗み出すケースが考えられます。
* 生成AIに対して大量の入出力結果を収集し、収集したデータをもとに「シャドウモデル」と呼ばれるモデルの複製を手元に作成することで、モデルを部分的に盗み出します。ただし大量の入出力結果を収集したとしても、本ケースで攻撃者が複製できるものは部分的なものであり、100%複製することはほぼ不可能です。
  * アクセス対してレートリミットを設定している場合、攻撃者はレートリミットを回避するために細工された悪意のあるリクエストを送信することが考えられます。
  * システムプロンプトなどで入出力の回答を限定している場合、攻撃者は制約を回避するためにプロンプトインジェクション攻撃などを利用することが考えられます。


## 対策
モデルの盗難には、下記のような対策があります。対策の原則として、一つの対策で十分なものはなく、多層防御することが大切です。

* 生成AIを提供しているシステムにおいて正しくアクセス制御と認証を用いて不正アクセスを防止します。
* 生成AIを提供しているシステム自体に適切な脆弱性管理を行い、システムの脆弱性によるリスクを防止します。
* アクセスログやモデルのリポジトリに対するアクセスをモニタリングし、不正な行動を検知・対応します。
* 生成AIを提供しているシステムのAPIに適切なレートリミットを設定し、データ流出のリスクを低減します。


## 参考資料
* OWASP Top 10 for LLM Applications v1.1 - OWASP
  * https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf
* AI Risk Management Framework - NIST　
  * https://www.nist.gov/itl/ai-risk-management-framework