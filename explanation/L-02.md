# L-02:モデルの盗難
## 概要
生成AIにおいてモデルとは、大量のデータを元に膨大なリソースをかけて学習したものであり、非常に重要な資産となり得ます。
モデルの盗難は、攻撃者が対象である生成AIモデルに不正アクセスすることで盗み出す行為、またはモデルの入出力結果をもとに、同等のものを手元に作成してしまう行為を指します。

## 攻撃例
ここでは、いくつかの例を挙げます。
* 生成AIを提供しているシステムの脆弱性を利用し、モデル本体に不正アクセスし、盗み出します。
* 攻撃者が細工したリクエストやプロンプトインジェクション等を用いて十分な数の出力を収集し、それをもとにシャドウモデルと呼ばれるモデルの複製を手元に作成します。
ただし、モデルの抽出においては、大量な出力が必要となります。また、攻撃者は複製できるものは部分的なものであり、100%複製することは不可能です。


## 対策
モデルの盗難に対する対策は下記の対策があります。
* 生成AIを提供しているシステムにおいて正しくアクセス制御と認証を用いて不正アクセスを防止します。
* 生成AIを提供しているシステム自体に適切な脆弱性管理を行い、システムの脆弱性によるリスクを防止します。
* アクセスログやモデルのリポジトリに対するアクセスをモニタリングし、不正な行動を検知・対応します。
* 生成AIを提供しているシステムのAPIに適切なレートリミットを設定し、データ流出のリスクを低減します。

## 参考資料
* OWASP Top 10 for LLM Applications v1.1 - OWASP
  * https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf
* AI Risk Management Framework - NIST　
  * https://www.nist.gov/itl/ai-risk-management-framework